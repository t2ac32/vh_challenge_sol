{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import math\n",
    "import datetime\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as bk\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, SeparableConv2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from librosa.feature import melspectrogram\n",
    "from librosa.display import specshow\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "#print(\"Num GPUs Available: \",len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "#TODO: export requirements\n",
    "\n",
    "audio_settings= {\n",
    "    \"sr\" : 44100,\n",
    "    \"n_fft\" : 2205,\n",
    "    \"hop_length\" : 441,\n",
    "    \"win_length\" : 442,\n",
    "    \"n_mels\" : 128,\n",
    "    \"fmin\" : 10,\n",
    "    \"fmax\" : 22050,\n",
    "    }\n",
    "r_settings= {\n",
    "    \"sr\" : 44100,\n",
    "    \"n_fft\" : 2205,\n",
    "    \"hop_length\" : 441,\n",
    "    \"win_length\" : 442,\n",
    "    \"n_mels\" : 128,\n",
    "    \"fmin\" : 10,\n",
    "    \"fmax\" : 22050,\n",
    "    }\n",
    "\n",
    "model_settings = {\n",
    "    'samplerate': 44100,\n",
    "    'n_mels': 128,\n",
    "    'fmin': 10,\n",
    "    'fmax': 22050,\n",
    "    'n_fft': 2205,\n",
    "    'hop_length': 441,\n",
    "    'frames': 500,\n",
    "    'batch':10,\n",
    "    'epochs': 80,\n",
    "    'train_samples': 1600,\n",
    "    'val_samples': 400,\n",
    "    'lr': 0.01,\n",
    "    'nesterov_momentum': 0.09\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in Data frame:  2000\n",
      "Number of Rows in dataframe which contain NaN in any column :  0\n",
      "Number of folds:  5\n",
      "Count per fold:  400\n",
      "Classes indexes in non categorical index:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-100038-A-14.wav</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-100210-A-36.wav</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-100210-B-36.wav</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-101296-A-19.wav</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  target\n",
       "0   1-100032-A-0.wav       0\n",
       "1  1-100038-A-14.wav      14\n",
       "2  1-100210-A-36.wav      36\n",
       "3  1-100210-B-36.wav      36\n",
       "4  1-101296-A-19.wav      19"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Dataset\n",
    "'''\n",
    "According to esc50 documentation rooster class is = 1.\n",
    "'''\n",
    "#model path\n",
    "\n",
    "MODEL_DIR = Path('model' + '/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")) \n",
    "TBOARD_LOGS = Path('tb_logs' + '/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "#50 CLASSES SOUND DATA SET\n",
    "AUDIO_DS_PATH = Path(\"Data/Dataset/audio\")\n",
    "DF_PATH =  Path(\"Data/Dataset/esc50.csv\")\n",
    "\n",
    "#Dataset as pandas dataframe\n",
    "sdf = pd.read_csv(DF_PATH)\n",
    "\n",
    "print(\"Number of entries in Data frame: \", len(sdf.index))\n",
    "# Count number of rows in a dataframe that contains NaN any column\n",
    "seriesObj = sdf.apply(lambda x: x.isnull().any(), axis=1)\n",
    "numOfRows = len(seriesObj[seriesObj == True].index)\n",
    "print('Number of Rows in dataframe which contain NaN in any column : ', numOfRows)\n",
    "\n",
    "#PRINT COUNT Fold SETS\n",
    "seriesObj = sdf.apply(lambda x: True if x['fold'] == 1 else False , axis=1)\n",
    "# Count number of True in series\n",
    "numOfRows = len(seriesObj[seriesObj == True].index)\n",
    "#target classes non categorical indexes easier to work with keras\n",
    "classes = sorted(sdf.target.unique())\n",
    "\n",
    "print('Number of folds: ', len(sdf.fold.unique()))\n",
    "print('Count per fold: ', numOfRows)\n",
    "print('Classes indexes in non categorical index: ', classes )\n",
    "\n",
    "#clean data set\n",
    "# Removed some columsn that don not seem important\n",
    "sdf = sdf.drop(['take','src_file', 'category','esc10','fold'], axis=1)\n",
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-101-4c9467ab3506>, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-101-4c9467ab3506>\"\u001b[1;36m, line \u001b[1;32m34\u001b[0m\n\u001b[1;33m    def load_audio_windows(audio_path:Path, audio_settings reshape: bool):\u001b[0m\n\u001b[1;37m                                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#kfold-spliting right now not using fold columns in dataset\n",
    "def split_data(dataframe):\n",
    "    train, test = train_test_split(dataframe, test_size=0.2)\n",
    "    return train, test\n",
    "    \n",
    "def get_fold_from(dataframe):\n",
    "    kf = KFold(n_splits = 5,shuffle=True, random_state=1)    \n",
    "    result = next(kf.split(dataframe), None)\n",
    "    train = dataframe.iloc[result[0]]\n",
    "    val =  dataframe.iloc[result[1]]\n",
    "    return train, val\n",
    "\n",
    "def get_audiop(audio_fn:str):\n",
    "    return AUDIO_DS_PATH / audio_fn\n",
    "\n",
    "def pre_process_stem(audio_path:Path):\n",
    "    y, sr = librosa.load(audio_path, 44100)\n",
    "    #audio_sample, _  = librosa.effects.trim(y)\n",
    "    return y\n",
    "\n",
    "def apply_log_db(windows):\n",
    "    DB = librosa.amplitude_to_db(windows, ref=np.max)\n",
    "    return DB\n",
    "\n",
    "def compute_windows(audio_stem, audio_settings):\n",
    "    windows = np.abs(librosa.stft(audio_stem,\n",
    "                                  n_fft=audio_settings['n_fft'],\n",
    "                                  hop_length=audio_settings['hop_length']\n",
    "                                 )\n",
    "                    )\n",
    "    return windows\n",
    "\n",
    "#mels will be the input for the network.\n",
    "def load_audio_windows(audio_path:Path, audio_settings reshape: bool):\n",
    "    audio_stem = pre_process_stem(audio_path)\n",
    "    mels = compute_melspect_for(audio_stem, audio_settings)\n",
    "    if  reshape:\n",
    "        mels = np.expand_dims(mels, axis=-1)\n",
    "    return mels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one liner Mel's Spectrogram\n",
    "def compute_melspect_for(audio_stem, audio_settings):\n",
    "    S = librosa.feature.melspectrogram(audio_stem, sr=audio_settings['sr'],\n",
    "                                       n_fft=audio_settings['n_fft'],\n",
    "                                       hop_length=audio_settings['hop_length'],\n",
    "                                       n_mels=audio_settings['n_mels'])\n",
    "    #S_DB = librosa.power_to_db(S, ref=np.max)\n",
    "    mellog = np.log(S + 1e-9)\n",
    "    melnormalized = librosa.util.normalize(mellog)\n",
    "    return melnormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute melspectrogram:\n",
    "#https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0\n",
    "#import librosa\n",
    "audio_sample_path  =  AUDIO_DS_PATH /\"1-100032-A-0.wav\"\n",
    "\n",
    "audio_stem = pre_process_stem(audio_sample_path)\n",
    "librosa.display.waveplot(audio_stem, sr=44100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply fourier transform\n",
    "t_n_fft = 2205 #window size = 50ms\n",
    "\n",
    "fft_stem = np.abs(librosa.stft(audio_stem[:t_n_fft],\n",
    "                               n_fft=t_n_fft,\n",
    "                               hop_length=t_n_fft+1))\n",
    "plt.plot(fft_stem);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "windows = compute_windows(audio_stem, audio_settings)\n",
    "print('windows shape: ', windows.shape)\n",
    "librosa.display.specshow(windows, sr=44100, x_axis='time', y_axis='linear');\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform: y-axi (frequency) to log-scale\n",
    "#color (amplitud) to Decibels\n",
    "db_scaled_stem = apply_log_db(windows)\n",
    "librosa.display.specshow(db_scaled_stem, sr=44100, hop_length=441, x_axis='time', y_axis='log');\n",
    "plt.colorbar(format='%+2.0f dB');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_n_mels = 128\n",
    "test_mel = librosa.filters.mel(sr= audio_settings['sr'], \n",
    "                               n_fft=t_n_fft, \n",
    "                               n_mels=t_n_mels\n",
    "                              )\n",
    "plt.plot(windows[:, 1]);\n",
    "plt.plot(test_mel.dot(windows[:, 1]));\n",
    "plt.legend(labels=['Hz', 'mel']);\n",
    "plt.title('One sampled window for example, before and after converting to mel.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute mel spectrogram\n",
    "\n",
    "\n",
    "#this patition the Hz scale into bins\n",
    "#also tranforms each bin into a correpsonding bin in the Mel scale, using\n",
    "#overlapping triangular filters.\n",
    "\n",
    "plt.figure(figsize=(15, 4));\n",
    "\n",
    "plt.subplot(1, 3, 1);\n",
    "librosa.display.specshow(test_mel, sr=r_settings['sr'],\n",
    "                         hop_length=r_settings['hop_length'], \n",
    "                         x_axis='linear');\n",
    "plt.ylabel('Mel filter');\n",
    "plt.colorbar();\n",
    "plt.title('1. Our filter bank for converting from Hz to mels.');\n",
    "\n",
    "plt.subplot(1, 3, 2);\n",
    "mel_10 = librosa.filters.mel(sr=r_settings['sr'],\n",
    "                             n_fft=r_settings['n_fft'],\n",
    "                             n_mels=r_settings['n_mels'])\n",
    "librosa.display.specshow(test_mel, sr=r_settings['sr'],\n",
    "                         hop_length=r_settings['hop_length'],\n",
    "                         x_axis='linear');\n",
    "plt.ylabel('Mel filter');\n",
    "plt.colorbar();\n",
    "plt.title('2. Easier to see what is happening with only 10 mels.');\n",
    "\n",
    "plt.subplot(1, 3, 3);\n",
    "idxs_to_plot = [0, 9, 49, 99, 127]\n",
    "for i in idxs_to_plot:\n",
    "    plt.plot(test_mel[i]);\n",
    "plt.legend(labels=[f'{i+1}' for i in idxs_to_plot]);\n",
    "plt.title('3. Plotting some triangular filters separately.');\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#n_fft : window size = 50ms must be power of 2 \n",
    "r_settings= {\n",
    "    \"sr\" : 44100,\n",
    "    \"n_fft\" : 2048,\n",
    "    \"hop_length\" : 441,\n",
    "    \"win_length\" : 442,\n",
    "    \"n_mels\" : 128,\n",
    "    \"fmin\" : 10,\n",
    "    \"fmax\" : 22050,\n",
    "}\n",
    "    \n",
    "mels = compute_melspect_for(audio_stem, r_settings)\n",
    "print('mels hsape:', mels.shape)\n",
    "librosa.display.specshow(mels, sr=r_settings['sr'],\n",
    "                         hop_length=r_settings['hop_length'],\n",
    "                         x_axis='time', y_axis='mel');\n",
    "plt.colorbar(format='%+2.0f dB');\n",
    "\n",
    "\n",
    "reshaped_input = mels.reshape(128, 501, 1)\n",
    "print(reshaped_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(windows[:,1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def build_model(frames=501, bands=128, channels=1, num_labels=50,\n",
    "                conv_size=(5,5), conv_block='conv',\n",
    "                downsample_size=(4,2),\n",
    "                fully_connected=64,\n",
    "                n_stages=None, n_blocks_per_stage=None,\n",
    "                filters=24, kernels_growth=2,\n",
    "                dropout=0.5,\n",
    "                use_strides=False):\n",
    "    \"\"\"\n",
    "    Implements SB-CNN model from\n",
    "    Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification\n",
    "    Salamon and Bello, 2016.\n",
    "    https://arxiv.org/pdf/1608.04363.pdf\n",
    "    Based on https://gist.github.com/jaron/5b17c9f37f351780744aefc74f93d3ae\n",
    "    but parameters are changed back to those of the original paper authors,\n",
    "    and added Batch Normalization\n",
    "    \"\"\"\n",
    "    print('Building Model')\n",
    "    Conv2 = SeparableConv2D if conv_block == 'depthwise_separable' else Convolution2D\n",
    "    assert conv_block in ('conv', 'depthwise_separable')\n",
    "    kernel = conv_size\n",
    "    if use_strides:\n",
    "        strides = downsample_size\n",
    "        pool = (1, 1)\n",
    "    else:\n",
    "        strides = (1, 1)\n",
    "        pool = downsample_size\n",
    "\n",
    "    block1 = [\n",
    "        Convolution2D(filters, kernel, padding='same', strides=strides,\n",
    "                      data_format='channels_last',\n",
    "                      input_shape=(bands, frames, channels)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=pool),\n",
    "        Activation('relu'),\n",
    "    ]\n",
    "    block2 = [\n",
    "        Conv2(filters*kernels_growth, kernel, padding='same', strides=strides),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=pool),\n",
    "        Activation('relu'),\n",
    "    ]\n",
    "    block3 = [\n",
    "        Conv2(filters*kernels_growth, kernel, padding='valid', strides=strides),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "    ]\n",
    "    backend = [\n",
    "        Flatten(),\n",
    "\n",
    "        Dropout(dropout),\n",
    "        Dense(fully_connected, kernel_regularizer=l2(0.001)),\n",
    "        Activation('relu'),\n",
    "\n",
    "        Dropout(dropout),\n",
    "        Dense(num_labels, kernel_regularizer=l2(0.001)),\n",
    "        Activation('softmax'),\n",
    "    ]\n",
    "    layers = block1 + block2 + block3 + backend\n",
    "    model = Sequential(layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_generator(data,audio_settings, batchsize):\n",
    "    \"\"\"\n",
    "    Keras generator for lazy-loading\n",
    "    data based on a pandas.DataFrame\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        idx = np.random.choice(len(data), size=batchsize, replace=False)\n",
    "\n",
    "        rows = data.iloc[idx, :].iterrows() #datailoc[idx, :].iterrows()[1]\n",
    "        mels = []\n",
    "        targets = []\n",
    "        for _, row in rows:\n",
    "            audio_p = get_audiop(row.filename) \n",
    "            mels.append(load_audio_windows(audio_p, audio_settings,True))\n",
    "            targets.append(row.target)\n",
    "        mels = np.asarray(mels)\n",
    "        categorical_targets = tf.keras.utils.to_categorical(targets, num_classes=50)\n",
    "        tf_ds = (mels, categorical_targets)\n",
    "        yield  tf_ds\n",
    "    '''\n",
    "    filename = row.filename\n",
    "    audio_path = get_audiop(filename)    \n",
    "    mels_arr = load_audio_windows(audio_path, audio_settings) \n",
    "    #reshape if needed\n",
    "    mels_arr = np.expand_dims(mels_arr, axis=-1)\n",
    "    batch = \n",
    "    target = row.target\n",
    "    tf_ds = (mels_arr, target) #bsize,n_mels,\n",
    "    '''\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dir(p: Path):\n",
    "    if p.is_dir():\n",
    "        print('models directory exists')\n",
    "    else:\n",
    "        p.mkdir(exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def train_model(out_dir,logs_dir, train, val, model, model_settings, audio_settings):\n",
    "    frame_samples = model_settings['hop_length']\n",
    "    window_frames = model_settings['frames']\n",
    "    epochs = model_settings['epochs']\n",
    "    batch_size = model_settings['batch']\n",
    "    lr = model_settings['lr']\n",
    "    momentum = model_settings['nesterov_momentum']\n",
    "    \n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(lr=lr, momentum=momentum,\n",
    "                                     nesterov=True)\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model_path = './' + str(out_dir) + \"/{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "    print('model_path: ',model_path)\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        model_path,\n",
    "        monitor='val_loss',\n",
    "        mode=\"auto\",\n",
    "        period=1,\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "    earlystop_callback = EarlyStopping(monitor='val_loss',\n",
    "                             patience=7,\n",
    "                            verbose=1,\n",
    "                            mode='auto')\n",
    "    logs_path ='./' + str(logs_dir)\n",
    "    tensorboard_callback = TensorBoard(log_dir=logs_path,\n",
    "                                       update_freq= 'epoch',\n",
    "                                       write_graph=True,\n",
    "                                       profile_batch=100000000)\n",
    "    \n",
    "    train_tfds = dataframe_generator(train, audio_settings, batch_size)\n",
    "    val_tfds = dataframe_generator(val, audio_settings, batch_size)\n",
    "\n",
    "    #tensorboard_callback\n",
    "    callbacks_list = [checkpoint,earlystop_callback,tensorboard_callback]\n",
    "    '''hist = model.fit_generator(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        steps_per_epoch=math.ceil(len(train) / batch_size),\n",
    "        validation_steps=math.ceil(len(val) / batch_size),\n",
    "        callbacks=callbacks_list,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "    )'''\n",
    "    \n",
    "    steps_per_epoch = len(train) // batch_size\n",
    "    validation_steps = len(val) // batch_size\n",
    "    \n",
    "    hist  = model.fit(x=train_tfds,\n",
    "                      epochs= epochs, verbose=1,\n",
    "                      callbacks=callbacks_list,\n",
    "                      steps_per_epoch= steps_per_epoch,\n",
    "                      validation_data= val_tfds ,\n",
    "                      validation_steps=validation_steps,\n",
    "                      shuffle=True, initial_epoch=0\n",
    "                     )\n",
    "    df = history_dataframe(hist)\n",
    "    history_path = os.path.join(out_dir, \"history.csv\")\n",
    "    df.to_csv(history_path)\n",
    "    \n",
    "    return hist\n",
    "    \n",
    "def history_dataframe(h):\n",
    "    data = {}\n",
    "    data[\"epoch\"] = h.epoch\n",
    "    for k, v in h.history.items():\n",
    "        data[k] = v\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def predict_model(settings, model, samples, loader, method=\"mean\", overlap=0.5):\n",
    "\n",
    "    out = []\n",
    "    for _, sample in samples.iterrows():\n",
    "        windows = load_windows(sample, settings, loader, overlap=overlap)\n",
    "        inputs = numpy.stack(windows)\n",
    "\n",
    "        predictions = model.predict(inputs)\n",
    "        if method == \"mean\":\n",
    "            p = numpy.mean(predictions, axis=0)\n",
    "            assert len(p) == 10\n",
    "            out.append(p)\n",
    "        elif method == \"majority\":\n",
    "            votes = numpy.argmax(predictions, axis=1)\n",
    "            p = numpy.bincount(votes, minlength=10) / len(votes)\n",
    "            out.append(p)\n",
    "\n",
    "    ret = numpy.stack(out)\n",
    "    assert len(ret.shape) == 2, ret.shape\n",
    "    assert ret.shape[0] == len(out), ret.shape\n",
    "    assert ret.shape[1] == 10, ret.shape  # classes\n",
    "\n",
    "    return ret\n",
    "\n",
    "def main():\n",
    "    name = \"vh_challenge\"\n",
    "    output_dir= check_dir(MODEL_DIR)\n",
    "    logs_dir = check_dir(TBOARD_LOGS)\n",
    "    #Check data exists\n",
    "    \n",
    "    train, val = split_data(sdf)\n",
    "    print('Setup:')\n",
    "    print('Train size: ', len(train))\n",
    "    print('Validation size: ', len (val))\n",
    "    print('Epochs: ', model_settings['epochs'])\n",
    "    \n",
    "    m = build_model()\n",
    "    m.summary()\n",
    "    print(\"Training model\", name)\n",
    "\n",
    "    t= train_model(output_dir,logs_dir, train, val, m, model_settings, r_settings)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:\n",
      "Train size:  1600\n",
      "Validation size:  400\n",
      "Epochs:  80\n",
      "Building Model\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 128, 501, 24)      624       \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 128, 501, 24)      96        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 32, 250, 24)       0         \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 32, 250, 24)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 32, 250, 48)       28848     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 32, 250, 48)       192       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 8, 125, 48)        0         \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 8, 125, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 4, 121, 48)        57648     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 4, 121, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 4, 121, 48)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 23232)             0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 23232)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                1486912   \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 50)                3250      \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 50)                0         \n",
      "=================================================================\n",
      "Total params: 1,577,762\n",
      "Trainable params: 1,577,522\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "Training model vh_challenge\n",
      "model_path:  ./model\\20200928-013914/{epoch:02d}-{val_accuracy:.2f}.hdf5\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 160 steps, validate for 40 steps\n",
      "Epoch 1/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 4.1617 - accuracy: 0.0252\n",
      "Epoch 00001: val_loss improved from inf to 4.09909, saving model to ./model\\20200928-013914/01-0.01.hdf5\n",
      "160/160 [==============================] - 69s 429ms/step - loss: 4.1614 - accuracy: 0.0250 - val_loss: 4.0991 - val_accuracy: 0.0125\n",
      "Epoch 2/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 4.0993 - accuracy: 0.0208\n",
      "Epoch 00002: val_loss improved from 4.09909 to 4.09845, saving model to ./model\\20200928-013914/02-0.01.hdf5\n",
      "160/160 [==============================] - 71s 441ms/step - loss: 4.0993 - accuracy: 0.0206 - val_loss: 4.0984 - val_accuracy: 0.0075\n",
      "Epoch 3/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 4.0969 - accuracy: 0.0189\n",
      "Epoch 00003: val_loss improved from 4.09845 to 4.09119, saving model to ./model\\20200928-013914/03-0.02.hdf5\n",
      "160/160 [==============================] - 69s 429ms/step - loss: 4.0968 - accuracy: 0.0188 - val_loss: 4.0912 - val_accuracy: 0.0225\n",
      "Epoch 4/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 4.0709 - accuracy: 0.0314\n",
      "Epoch 00004: val_loss improved from 4.09119 to 4.02011, saving model to ./model\\20200928-013914/04-0.03.hdf5\n",
      "160/160 [==============================] - 66s 412ms/step - loss: 4.0707 - accuracy: 0.0312 - val_loss: 4.0201 - val_accuracy: 0.0250\n",
      "Epoch 5/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 4.0401 - accuracy: 0.0365\n",
      "Epoch 00005: val_loss improved from 4.02011 to 3.90951, saving model to ./model\\20200928-013914/05-0.05.hdf5\n",
      "160/160 [==============================] - 67s 416ms/step - loss: 4.0400 - accuracy: 0.0369 - val_loss: 3.9095 - val_accuracy: 0.0525\n",
      "Epoch 6/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 3.9980 - accuracy: 0.0478\n",
      "Epoch 00006: val_loss did not improve from 3.90951\n",
      "160/160 [==============================] - 69s 432ms/step - loss: 3.9997 - accuracy: 0.0475 - val_loss: 3.9813 - val_accuracy: 0.0350\n",
      "Epoch 7/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 3.9271 - accuracy: 0.0597\n",
      "Epoch 00007: val_loss improved from 3.90951 to 3.79578, saving model to ./model\\20200928-013914/07-0.04.hdf5\n",
      "160/160 [==============================] - 66s 411ms/step - loss: 3.9263 - accuracy: 0.0594 - val_loss: 3.7958 - val_accuracy: 0.0400\n",
      "Epoch 8/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 3.9217 - accuracy: 0.0535\n",
      "Epoch 00008: val_loss improved from 3.79578 to 3.75183, saving model to ./model\\20200928-013914/08-0.07.hdf5\n",
      "160/160 [==============================] - 66s 413ms/step - loss: 3.9222 - accuracy: 0.0531 - val_loss: 3.7518 - val_accuracy: 0.0725\n",
      "Epoch 9/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 3.8849 - accuracy: 0.0635\n",
      "Epoch 00009: val_loss did not improve from 3.75183\n",
      "160/160 [==============================] - 65s 409ms/step - loss: 3.8848 - accuracy: 0.0631 - val_loss: 4.1390 - val_accuracy: 0.0225\n",
      "Epoch 10/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 3.8639 - accuracy: 0.0597\n",
      "Epoch 00010: val_loss improved from 3.75183 to 3.74669, saving model to ./model\\20200928-013914/10-0.10.hdf5\n",
      "160/160 [==============================] - 67s 419ms/step - loss: 3.8659 - accuracy: 0.0594 - val_loss: 3.7467 - val_accuracy: 0.1000\n",
      "Epoch 11/80\n",
      " 27/160 [====>.........................] - ETA: 39s - loss: 3.7765 - accuracy: 0.0667"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%kill` not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
