{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import math\n",
    "import datetime\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as bk\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard \n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, SeparableConv2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from librosa.feature import melspectrogram\n",
    "from librosa.display import specshow\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "#print(\"Num GPUs Available: \",len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "#TODO: export requirements\n",
    "\n",
    "audio_settings= {\n",
    "    \"sr\" : 44100,\n",
    "    \"n_fft\" : 2205,\n",
    "    \"hop_length\" : 441,\n",
    "    \"win_length\" : 442,\n",
    "    \"n_mels\" : 128,\n",
    "    \"fmin\" : 10,\n",
    "    \"fmax\" : 22050,\n",
    "    }\n",
    "r_settings= {\n",
    "    \"sr\" : 44100,\n",
    "    \"n_fft\" : 2205,\n",
    "    \"hop_length\" : 441,\n",
    "    \"win_length\" : 442,\n",
    "    \"n_mels\" : 128,\n",
    "    \"fmin\" : 10,\n",
    "    \"fmax\" : 22050,\n",
    "    }\n",
    "\n",
    "model_settings = {\n",
    "    'samplerate': 44100,\n",
    "    'n_mels': 128,\n",
    "    'fmin': 10,\n",
    "    'fmax': 22050,\n",
    "    'n_fft': 2205,\n",
    "    'hop_length': 441,\n",
    "    'frames': 500,\n",
    "    'batch':10,\n",
    "    'epochs': 80,\n",
    "    'train_samples': 1600,\n",
    "    'val_samples': 400,\n",
    "    'lr': 0.01,\n",
    "    'nesterov_momentum': 0.09\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in Data frame:  2000\n",
      "Number of Rows in dataframe which contain NaN in any column :  0\n",
      "Number of folds:  5\n",
      "Count per fold:  400\n",
      "Classes indexes in non categorical index:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-100038-A-14.wav</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-100210-A-36.wav</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-100210-B-36.wav</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-101296-A-19.wav</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  target\n",
       "0   1-100032-A-0.wav       0\n",
       "1  1-100038-A-14.wav      14\n",
       "2  1-100210-A-36.wav      36\n",
       "3  1-100210-B-36.wav      36\n",
       "4  1-101296-A-19.wav      19"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Dataset\n",
    "'''\n",
    "According to esc50 documentation rooster class is = 1.\n",
    "'''\n",
    "#model path\n",
    "\n",
    "MODEL_DIR = Path('model' + '/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")) \n",
    "TBOARD_LOGS = Path('tb_logs' + '/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "#50 CLASSES SOUND DATA SET\n",
    "AUDIO_DS_PATH = Path(\"Data/Dataset/audio\")\n",
    "DF_PATH =  Path(\"Data/Dataset/esc50.csv\")\n",
    "\n",
    "#Dataset as pandas dataframe\n",
    "sdf = pd.read_csv(DF_PATH)\n",
    "\n",
    "print(\"Number of entries in Data frame: \", len(sdf.index))\n",
    "# Count number of rows in a dataframe that contains NaN any column\n",
    "seriesObj = sdf.apply(lambda x: x.isnull().any(), axis=1)\n",
    "numOfRows = len(seriesObj[seriesObj == True].index)\n",
    "print('Number of Rows in dataframe which contain NaN in any column : ', numOfRows)\n",
    "\n",
    "#PRINT COUNT Fold SETS\n",
    "seriesObj = sdf.apply(lambda x: True if x['fold'] == 1 else False , axis=1)\n",
    "# Count number of True in series\n",
    "numOfRows = len(seriesObj[seriesObj == True].index)\n",
    "#target classes non categorical indexes easier to work with keras\n",
    "classes = sorted(sdf.target.unique())\n",
    "\n",
    "print('Number of folds: ', len(sdf.fold.unique()))\n",
    "print('Count per fold: ', numOfRows)\n",
    "print('Classes indexes in non categorical index: ', classes )\n",
    "\n",
    "#clean data set\n",
    "# Removed some columsn that don not seem important\n",
    "sdf = sdf.drop(['take','src_file', 'category','esc10','fold'], axis=1)\n",
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kfold-spliting right now not using fold columns in dataset\n",
    "def split_data(dataframe):\n",
    "    train, test = train_test_split(dataframe, test_size=0.2)\n",
    "    return train, test\n",
    "    \n",
    "def get_fold_from(dataframe):\n",
    "    kf = KFold(n_splits = 5,shuffle=True, random_state=1)    \n",
    "    result = next(kf.split(dataframe), None)\n",
    "    train = dataframe.iloc[result[0]]\n",
    "    val =  dataframe.iloc[result[1]]\n",
    "    return train, val\n",
    "\n",
    "def get_audiop(audio_fn:str):\n",
    "    return AUDIO_DS_PATH / audio_fn\n",
    "\n",
    "def pre_process_stem(audio_path:Path):\n",
    "    y, sr = librosa.load(audio_path, 44100)\n",
    "    #audio_sample, _  = librosa.effects.trim(y)\n",
    "    return y\n",
    "\n",
    "def apply_log_db(windows):\n",
    "    DB = librosa.amplitude_to_db(windows, ref=np.max)\n",
    "    return DB\n",
    "\n",
    "def compute_windows(audio_stem, audio_settings):\n",
    "    windows = np.abs(librosa.stft(audio_stem,\n",
    "                                  n_fft=audio_settings['n_fft'],\n",
    "                                  hop_length=audio_settings['hop_length']\n",
    "                                 )\n",
    "                    )\n",
    "    return windows\n",
    "\n",
    "#mels will be the input for the network.\n",
    "#TODO: Rename to load_audio_windows\n",
    "def load_audio_windows(audio_path:Path, audio_settings, reshape: bool):\n",
    "    audio_stem = pre_process_stem(audio_path)\n",
    "    mels = compute_melspect_for(audio_stem, audio_settings)\n",
    "    if  reshape:\n",
    "        mels = np.expand_dims(mels, axis=-1)\n",
    "    return mels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one liner Mel's Spectrogram\n",
    "def compute_melspect_for(audio_stem, audio_settings):\n",
    "    S = librosa.feature.melspectrogram(audio_stem, sr=audio_settings['sr'],\n",
    "                                       n_fft=audio_settings['n_fft'],\n",
    "                                       hop_length=audio_settings['hop_length'],\n",
    "                                       n_mels=audio_settings['n_mels'])\n",
    "    #S_DB = librosa.power_to_db(S, ref=np.max)\n",
    "    mellog = np.log(S + 1e-9)\n",
    "    melnormalized = librosa.util.normalize(mellog)\n",
    "    return melnormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def build_model(frames=501, bands=128, channels=1, num_labels=50,\n",
    "                conv_size=(5,5), conv_block='conv',\n",
    "                downsample_size=(4,2),\n",
    "                fully_connected=64,\n",
    "                n_stages=None, n_blocks_per_stage=None,\n",
    "                filters=24, kernels_growth=2,\n",
    "                dropout=0.5,\n",
    "                use_strides=False):\n",
    "    \"\"\"\n",
    "    Implements SB-CNN model from\n",
    "    Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification\n",
    "    Salamon and Bello, 2016.\n",
    "    https://arxiv.org/pdf/1608.04363.pdf\n",
    "    Based on https://gist.github.com/jaron/5b17c9f37f351780744aefc74f93d3ae\n",
    "    but parameters are changed back to those of the original paper authors,\n",
    "    and added Batch Normalization\n",
    "    \"\"\"\n",
    "    print('Building Model')\n",
    "    Conv2 = SeparableConv2D if conv_block == 'depthwise_separable' else Convolution2D\n",
    "    assert conv_block in ('conv', 'depthwise_separable')\n",
    "    kernel = conv_size\n",
    "    if use_strides:\n",
    "        strides = downsample_size\n",
    "        pool = (1, 1)\n",
    "    else:\n",
    "        strides = (1, 1)\n",
    "        pool = downsample_size\n",
    "\n",
    "    block1 = [\n",
    "        Convolution2D(filters, kernel, padding='same', strides=strides,\n",
    "                      data_format='channels_last',\n",
    "                      input_shape=(bands, frames, channels)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=pool),\n",
    "        Activation('relu'),\n",
    "    ]\n",
    "    block2 = [\n",
    "        Conv2(filters*kernels_growth, kernel, padding='same', strides=strides),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=pool),\n",
    "        Activation('relu'),\n",
    "    ]\n",
    "    block3 = [\n",
    "        Conv2(filters*kernels_growth, kernel, padding='valid', strides=strides),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "    ]\n",
    "    backend = [\n",
    "        Flatten(),\n",
    "\n",
    "        Dropout(dropout),\n",
    "        Dense(fully_connected, kernel_regularizer=l2(0.001)),\n",
    "        Activation('relu'),\n",
    "\n",
    "        Dropout(dropout),\n",
    "        Dense(num_labels, kernel_regularizer=l2(0.001)),\n",
    "        Activation('softmax'),\n",
    "    ]\n",
    "    layers = block1 + block2 + block3 + backend\n",
    "    model = Sequential(layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_generator(data,audio_settings, batchsize):\n",
    "    \"\"\"\n",
    "    Keras generator for lazy-loading\n",
    "    data based on a pandas.DataFrame\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        idx = np.random.choice(len(data), size=batchsize, replace=False)\n",
    "\n",
    "        rows = data.iloc[idx, :].iterrows() #datailoc[idx, :].iterrows()[1]\n",
    "        mels = []\n",
    "        targets = []\n",
    "        for _, row in rows:\n",
    "            audio_p = get_audiop(row.filename) \n",
    "            mels.append(load_audio_windows(audio_p, audio_settings,True))\n",
    "            targets.append(row.target)\n",
    "        mels = np.asarray(mels)\n",
    "        categorical_targets = tf.keras.utils.to_categorical(targets, num_classes=50)\n",
    "        tf_ds = (mels, categorical_targets)\n",
    "        yield  tf_ds\n",
    "    '''\n",
    "    filename = row.filename\n",
    "    audio_path = get_audiop(filename)    \n",
    "    mels_arr = load_audio_windows(audio_path, audio_settings) \n",
    "    #reshape if needed\n",
    "    mels_arr = np.expand_dims(mels_arr, axis=-1)\n",
    "    batch = \n",
    "    target = row.target\n",
    "    tf_ds = (mels_arr, target) #bsize,n_mels,\n",
    "    '''\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dir(p: Path):\n",
    "    if p.is_dir():\n",
    "        print('models directory exists')\n",
    "    else:\n",
    "        p.mkdir(exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def train_model(out_dir,logs_dir, train, val, model, model_settings, audio_settings):\n",
    "    frame_samples = model_settings['hop_length']\n",
    "    window_frames = model_settings['frames']\n",
    "    epochs = model_settings['epochs']\n",
    "    batch_size = model_settings['batch']\n",
    "    lr = model_settings['lr']\n",
    "    momentum = model_settings['nesterov_momentum']\n",
    "    \n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(lr=lr, momentum=momentum,\n",
    "                                     nesterov=True)\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model_path = './' + str(out_dir) + \"/{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "    print('model_path: ',model_path)\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        model_path,\n",
    "        monitor='val_loss',\n",
    "        mode=\"auto\",\n",
    "        period=1,\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "    earlystop_callback = EarlyStopping(monitor='val_loss',\n",
    "                             patience=7,\n",
    "                            verbose=1,\n",
    "                            mode='auto')\n",
    "    logs_path ='./' + str(logs_dir)\n",
    "    tensorboard_callback = TensorBoard(log_dir=logs_path,\n",
    "                                       update_freq= 'epoch',\n",
    "                                       write_graph=True,\n",
    "                                       profile_batch=100000000)\n",
    "    lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "    \n",
    "    train_tfds = dataframe_generator(train, audio_settings, batch_size)\n",
    "    val_tfds = dataframe_generator(val, audio_settings, batch_size)\n",
    "\n",
    "    #tensorboard_callback\n",
    "    callbacks_list = [checkpoint,earlystop_callback,tensorboard_callback,lr_callback]\n",
    "    '''hist = model.fit_generator(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        steps_per_epoch=math.ceil(len(train) / batch_size),\n",
    "        validation_steps=math.ceil(len(val) / batch_size),\n",
    "        callbacks=callbacks_list,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "    )'''\n",
    "    \n",
    "    steps_per_epoch = len(train) // batch_size\n",
    "    validation_steps = len(val) // batch_size\n",
    "    \n",
    "    hist  = model.fit(x=train_tfds,\n",
    "                      epochs= epochs, verbose=1,\n",
    "                      callbacks=callbacks_list,\n",
    "                      steps_per_epoch= steps_per_epoch,\n",
    "                      validation_data= val_tfds ,\n",
    "                      validation_steps=validation_steps,\n",
    "                      shuffle=True, initial_epoch=0\n",
    "                     )\n",
    "    df = history_dataframe(hist)\n",
    "    history_path = os.path.join(out_dir, \"history.csv\")\n",
    "    df.to_csv(history_path)\n",
    "    \n",
    "    return hist\n",
    "    \n",
    "def history_dataframe(h):\n",
    "    data = {}\n",
    "    data[\"epoch\"] = h.epoch\n",
    "    for k, v in h.history.items():\n",
    "        data[k] = v\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def predict_model( sample_path, audio_settings, model, method=\"mean\"):\n",
    "\n",
    "    out = []\n",
    "    track = load_audio_window(audio_path, audio_settings, True)\n",
    "    # shape (128,15214)\n",
    "    \n",
    "    for window_step in range(250,track.shape[1],250):\n",
    "        s_init = window_step - 250\n",
    "        s_end = window_step + 250\n",
    "        sample = track[:,s_init:s_end,:]\n",
    "        \n",
    "        y_predict = model.predict(sample)\n",
    "        if method == \"mean\":\n",
    "            p = numpy.mean(predictions, axis=0)\n",
    "            assert len(p) == 10\n",
    "            out.append(p)\n",
    "        elif method == \"majority\":\n",
    "            votes = numpy.argmax(predictions, axis=1)\n",
    "            p = numpy.bincount(votes, minlength=10) / len(votes)\n",
    "            out.append(p)\n",
    "    \n",
    "\n",
    "    ret = numpy.stack(out)\n",
    "    print(ret)\n",
    "    assert len(ret.shape) == 2, ret.shape\n",
    "    assert ret.shape[0] == len(out), ret.shape\n",
    "    assert ret.shape[1] == 50, ret.shape  # classes\n",
    "\n",
    "    return ret\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)    \n",
    "\n",
    "def main():\n",
    "    name = \"vh_challenge\"\n",
    "    output_dir= check_dir(MODEL_DIR)\n",
    "    logs_dir = check_dir(TBOARD_LOGS)\n",
    "    #Check data exists\n",
    "    \n",
    "    train, val = split_data(sdf)\n",
    "    print('Setup:')\n",
    "    print('Train size: ', len(train))\n",
    "    print('Validation size: ', len (val))\n",
    "    print('Epochs: ', model_settings['epochs'])\n",
    "    \n",
    "    m = build_model()\n",
    "    m.summary()\n",
    "    print(\"Training model\", name)\n",
    "\n",
    "    t= train_model(output_dir,logs_dir, train, val, m, model_settings, r_settings)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup:\n",
      "Train size:  1600\n",
      "Validation size:  400\n",
      "Epochs:  80\n",
      "Building Model\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_33 (Conv2D)           (None, 128, 501, 24)      624       \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 128, 501, 24)      96        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 32, 250, 24)       0         \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 32, 250, 24)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 32, 250, 48)       28848     \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 32, 250, 48)       192       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 8, 125, 48)        0         \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 8, 125, 48)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 4, 121, 48)        57648     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 4, 121, 48)        192       \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 4, 121, 48)        0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 23232)             0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 23232)             0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                1486912   \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 50)                3250      \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 50)                0         \n",
      "=================================================================\n",
      "Total params: 1,577,762\n",
      "Trainable params: 1,577,522\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "Training model vh_challenge\n",
      "model_path:  ./model\\20200928-064222/{epoch:02d}-{val_accuracy:.2f}.hdf5\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 160 steps, validate for 40 steps\n",
      "Epoch 1/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 4.1533 - accuracy: 0.0208\n",
      "Epoch 00001: val_loss improved from inf to 4.09607, saving model to ./model\\20200928-064222/01-0.01.hdf5\n",
      "160/160 [==============================] - 68s 428ms/step - loss: 4.1529 - accuracy: 0.0206 - val_loss: 4.0961 - val_accuracy: 0.0125\n",
      "Epoch 2/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 4.0951 - accuracy: 0.0201\n",
      "Epoch 00002: val_loss improved from 4.09607 to 4.09558, saving model to ./model\\20200928-064222/02-0.03.hdf5\n",
      "160/160 [==============================] - 59s 372ms/step - loss: 4.0949 - accuracy: 0.0200 - val_loss: 4.0956 - val_accuracy: 0.0275\n",
      "Epoch 3/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 4.0941 - accuracy: 0.0195\n",
      "Epoch 00003: val_loss improved from 4.09558 to 4.09499, saving model to ./model\\20200928-064222/03-0.00.hdf5\n",
      "160/160 [==============================] - 59s 368ms/step - loss: 4.0941 - accuracy: 0.0194 - val_loss: 4.0950 - val_accuracy: 0.0050\n",
      "Epoch 4/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 4.0886 - accuracy: 0.0233\n",
      "Epoch 00004: val_loss improved from 4.09499 to 4.08741, saving model to ./model\\20200928-064222/04-0.01.hdf5\n",
      "160/160 [==============================] - 61s 383ms/step - loss: 4.0885 - accuracy: 0.0244 - val_loss: 4.0874 - val_accuracy: 0.0125\n",
      "Epoch 5/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 4.0865 - accuracy: 0.0226\n",
      "Epoch 00005: val_loss improved from 4.08741 to 4.06705, saving model to ./model\\20200928-064222/05-0.03.hdf5\n",
      "160/160 [==============================] - 61s 380ms/step - loss: 4.0866 - accuracy: 0.0231 - val_loss: 4.0671 - val_accuracy: 0.0250\n",
      "Epoch 6/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 4.0632 - accuracy: 0.0346\n",
      "Epoch 00006: val_loss improved from 4.06705 to 4.05250, saving model to ./model\\20200928-064222/06-0.02.hdf5\n",
      "160/160 [==============================] - 60s 373ms/step - loss: 4.0634 - accuracy: 0.0344 - val_loss: 4.0525 - val_accuracy: 0.0225\n",
      "Epoch 7/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 4.0600 - accuracy: 0.0384\n",
      "Epoch 00007: val_loss did not improve from 4.05250\n",
      "160/160 [==============================] - 60s 374ms/step - loss: 4.0602 - accuracy: 0.0388 - val_loss: 4.0588 - val_accuracy: 0.0225\n",
      "Epoch 8/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 4.0387 - accuracy: 0.0396\n",
      "Epoch 00008: val_loss improved from 4.05250 to 3.93957, saving model to ./model\\20200928-064222/08-0.04.hdf5\n",
      "160/160 [==============================] - 60s 373ms/step - loss: 4.0378 - accuracy: 0.0406 - val_loss: 3.9396 - val_accuracy: 0.0425\n",
      "Epoch 9/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 4.0020 - accuracy: 0.0415\n",
      "Epoch 00009: val_loss improved from 3.93957 to 3.91913, saving model to ./model\\20200928-064222/09-0.03.hdf5\n",
      "160/160 [==============================] - 58s 362ms/step - loss: 4.0010 - accuracy: 0.0413 - val_loss: 3.9191 - val_accuracy: 0.0250\n",
      "Epoch 10/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 3.9713 - accuracy: 0.0560\n",
      "Epoch 00010: val_loss improved from 3.91913 to 3.82369, saving model to ./model\\20200928-064222/10-0.07.hdf5\n",
      "160/160 [==============================] - 57s 358ms/step - loss: 3.9713 - accuracy: 0.0556 - val_loss: 3.8237 - val_accuracy: 0.0725\n",
      "Epoch 11/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 3.9261 - accuracy: 0.0591\n",
      "Epoch 00011: val_loss improved from 3.82369 to 3.75122, saving model to ./model\\20200928-064222/11-0.09.hdf5\n",
      "160/160 [==============================] - 61s 380ms/step - loss: 3.9248 - accuracy: 0.0594 - val_loss: 3.7512 - val_accuracy: 0.0925\n",
      "Epoch 12/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 3.8809 - accuracy: 0.0509\n",
      "Epoch 00012: val_loss did not improve from 3.75122\n",
      "160/160 [==============================] - 59s 370ms/step - loss: 3.8839 - accuracy: 0.0506 - val_loss: 3.7991 - val_accuracy: 0.0525\n",
      "Epoch 13/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 3.7760 - accuracy: 0.0698\n",
      "Epoch 00013: val_loss improved from 3.75122 to 3.55008, saving model to ./model\\20200928-064222/13-0.10.hdf5\n",
      "160/160 [==============================] - 56s 352ms/step - loss: 3.7761 - accuracy: 0.0694 - val_loss: 3.5501 - val_accuracy: 0.1025\n",
      "Epoch 14/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 3.7738 - accuracy: 0.0560\n",
      "Epoch 00014: val_loss did not improve from 3.55008\n",
      "160/160 [==============================] - 59s 370ms/step - loss: 3.7742 - accuracy: 0.0556 - val_loss: 3.5650 - val_accuracy: 0.1100\n",
      "Epoch 15/80\n",
      "159/160 [============================>.] - ETA: 0s - loss: 3.7634 - accuracy: 0.0553\n",
      "Epoch 00015: val_loss did not improve from 3.55008\n",
      "160/160 [==============================] - 56s 351ms/step - loss: 3.7643 - accuracy: 0.0556 - val_loss: 3.5516 - val_accuracy: 0.1275\n",
      "Epoch 16/80\n",
      " 24/160 [===>..........................] - ETA: 32s - loss: 3.7991 - accuracy: 0.0750"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%kill` not found.\n"
     ]
    }
   ],
   "source": [
    "competition_track_path = Path('rooster_competition.wav')\n",
    "model_path = Path('/model/20200928-013914/43-0.41.hdf5')\n",
    "loaded_model = keras.models.load_model(model_path)\n",
    "prediction = predict_model( competition_track_path , audio_settings, loaded_model, method=\"mean\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
