{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import math\n",
    "import datetime\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as bk\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard \n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, SeparableConv2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from librosa.feature import melspectrogram\n",
    "from librosa.display import specshow\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "#print(\"Num GPUs Available: \",len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "#TODO: export requirements\n",
    "\n",
    "audio_settings= {\n",
    "    \"sr\" : 44100,\n",
    "    \"n_fft\" : 2205,\n",
    "    \"hop_length\" : 441,\n",
    "    \"win_length\" : 442,\n",
    "    \"n_mels\" : 128,\n",
    "    \"fmin\" : 10,\n",
    "    \"fmax\" : 22050,\n",
    "    }\n",
    "r_settings= {\n",
    "    \"sr\" : 44100,\n",
    "    \"n_fft\" : 2205,\n",
    "    \"hop_length\" : 441,\n",
    "    \"win_length\" : 442,\n",
    "    \"n_mels\" : 128,\n",
    "    \"fmin\" : 10,\n",
    "    \"fmax\" : 22050,\n",
    "    }\n",
    "\n",
    "model_settings = {\n",
    "    'samplerate': 44100,\n",
    "    'n_mels': 128,\n",
    "    'fmin': 10,\n",
    "    'fmax': 22050,\n",
    "    'n_fft': 2205,\n",
    "    'hop_length': 441,\n",
    "    'frames': 500,\n",
    "    'batch':10,\n",
    "    'epochs': 80,\n",
    "    'train_samples': 1600,\n",
    "    'val_samples': 400,\n",
    "    'lr': 0.01,\n",
    "    'nesterov_momentum': 0.09\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in Data frame:  2000\n",
      "Number of Rows in dataframe which contain NaN in any column :  0\n",
      "Number of folds:  5\n",
      "Count per fold:  400\n",
      "Classes indexes in non categorical index:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100032-A-0.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-100038-A-14.wav</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-100210-A-36.wav</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-100210-B-36.wav</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-101296-A-19.wav</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename  target\n",
       "0   1-100032-A-0.wav       0\n",
       "1  1-100038-A-14.wav      14\n",
       "2  1-100210-A-36.wav      36\n",
       "3  1-100210-B-36.wav      36\n",
       "4  1-101296-A-19.wav      19"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Dataset\n",
    "'''\n",
    "According to esc50 documentation rooster class is = 1.\n",
    "'''\n",
    "#model path\n",
    "\n",
    "MODEL_DIR = Path('model' + '/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")) \n",
    "TBOARD_LOGS = Path('tb_logs' + '/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "#50 CLASSES SOUND DATA SET\n",
    "AUDIO_DS_PATH = Path(\"Data/Dataset/audio\")\n",
    "DF_PATH =  Path(\"Data/Dataset/esc50.csv\")\n",
    "\n",
    "#Dataset as pandas dataframe\n",
    "sdf = pd.read_csv(DF_PATH)\n",
    "\n",
    "print(\"Number of entries in Data frame: \", len(sdf.index))\n",
    "# Count number of rows in a dataframe that contains NaN any column\n",
    "seriesObj = sdf.apply(lambda x: x.isnull().any(), axis=1)\n",
    "numOfRows = len(seriesObj[seriesObj == True].index)\n",
    "print('Number of Rows in dataframe which contain NaN in any column : ', numOfRows)\n",
    "\n",
    "#PRINT COUNT Fold SETS\n",
    "seriesObj = sdf.apply(lambda x: True if x['fold'] == 1 else False , axis=1)\n",
    "# Count number of True in series\n",
    "numOfRows = len(seriesObj[seriesObj == True].index)\n",
    "#target classes non categorical indexes easier to work with keras\n",
    "classes = sorted(sdf.target.unique())\n",
    "\n",
    "print('Number of folds: ', len(sdf.fold.unique()))\n",
    "print('Count per fold: ', numOfRows)\n",
    "print('Classes indexes in non categorical index: ', classes )\n",
    "\n",
    "#clean data set\n",
    "# Removed some columsn that don not seem important\n",
    "sdf = sdf.drop(['take','src_file', 'category','esc10','fold'], axis=1)\n",
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kfold-spliting right now not using fold columns in dataset\n",
    "def split_data(dataframe):\n",
    "    train, test = train_test_split(dataframe, test_size=0.2)\n",
    "    return train, test\n",
    "    \n",
    "def get_fold_from(dataframe):\n",
    "    kf = KFold(n_splits = 5,shuffle=True, random_state=1)    \n",
    "    result = next(kf.split(dataframe), None)\n",
    "    train = dataframe.iloc[result[0]]\n",
    "    val =  dataframe.iloc[result[1]]\n",
    "    return train, val\n",
    "\n",
    "def get_audiop(audio_fn:str):\n",
    "    return AUDIO_DS_PATH / audio_fn\n",
    "\n",
    "def pre_process_stem(audio_path:Path):\n",
    "    y, sr = librosa.load(audio_path, 44100)\n",
    "    #audio_sample, _  = librosa.effects.trim(y)\n",
    "    return y\n",
    "\n",
    "def apply_log_db(windows):\n",
    "    DB = librosa.amplitude_to_db(windows, ref=np.max)\n",
    "    return DB\n",
    "\n",
    "def compute_windows(audio_stem, audio_settings):\n",
    "    windows = np.abs(librosa.stft(audio_stem,\n",
    "                                  n_fft=audio_settings['n_fft'],\n",
    "                                  hop_length=audio_settings['hop_length']\n",
    "                                 )\n",
    "                    )\n",
    "    return windows\n",
    "\n",
    "#mels will be the input for the network.\n",
    "#TODO: Rename to load_audio_windows\n",
    "def load_audio_windows(audio_path:Path, audio_settings, reshape: bool):\n",
    "    audio_stem = pre_process_stem(audio_path)\n",
    "    mels = compute_melspect_for(audio_stem, audio_settings)\n",
    "    if  reshape:\n",
    "        mels = np.expand_dims(mels, axis=-1)\n",
    "    return mels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one liner Mel's Spectrogram\n",
    "def compute_melspect_for(audio_stem, audio_settings):\n",
    "    S = librosa.feature.melspectrogram(audio_stem, sr=audio_settings['sr'],\n",
    "                                       n_fft=audio_settings['n_fft'],\n",
    "                                       hop_length=audio_settings['hop_length'],\n",
    "                                       n_mels=audio_settings['n_mels'])\n",
    "    #S_DB = librosa.power_to_db(S, ref=np.max)\n",
    "    mellog = np.log(S + 1e-9)\n",
    "    melnormalized = librosa.util.normalize(mellog)\n",
    "    return melnormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def build_model(frames=501, bands=128, channels=1, num_labels=50,\n",
    "                conv_size=(5,5), conv_block='conv',\n",
    "                downsample_size=(4,2),\n",
    "                fully_connected=64,\n",
    "                n_stages=None, n_blocks_per_stage=None,\n",
    "                filters=24, kernels_growth=2,\n",
    "                dropout=0.5,\n",
    "                use_strides=False):\n",
    "    \"\"\"\n",
    "    Implements SB-CNN model from\n",
    "    Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification\n",
    "    Salamon and Bello, 2016.\n",
    "    https://arxiv.org/pdf/1608.04363.pdf\n",
    "    Based on https://gist.github.com/jaron/5b17c9f37f351780744aefc74f93d3ae\n",
    "    but parameters are changed back to those of the original paper authors,\n",
    "    and added Batch Normalization\n",
    "    \"\"\"\n",
    "    print('Building Model')\n",
    "    Conv2 = SeparableConv2D if conv_block == 'depthwise_separable' else Convolution2D\n",
    "    assert conv_block in ('conv', 'depthwise_separable')\n",
    "    kernel = conv_size\n",
    "    if use_strides:\n",
    "        strides = downsample_size\n",
    "        pool = (1, 1)\n",
    "    else:\n",
    "        strides = (1, 1)\n",
    "        pool = downsample_size\n",
    "\n",
    "    block1 = [\n",
    "        Convolution2D(filters, kernel, padding='same', strides=strides,\n",
    "                      data_format='channels_last',\n",
    "                      input_shape=(bands, frames, channels)),\n",
    "        BatchNormalization(axis=-1),\n",
    "        MaxPooling2D(pool_size=pool),\n",
    "        Activation('relu'),\n",
    "    ]\n",
    "    block2 = [\n",
    "        Conv2(filters*kernels_growth, kernel, padding='same', strides=strides),\n",
    "        BatchNormalization(axis=-1),\n",
    "        MaxPooling2D(pool_size=pool),\n",
    "        Activation('relu'),\n",
    "    ]\n",
    "    block3 = [\n",
    "        Conv2(filters*kernels_growth, kernel, padding='valid', strides=strides),\n",
    "        BatchNormalization(axis=-1),\n",
    "        Activation('relu'),\n",
    "    ]\n",
    "    backend = [\n",
    "        Flatten(),\n",
    "\n",
    "        Dropout(dropout),\n",
    "        Dense(fully_connected, kernel_regularizer=l2(0.001)),\n",
    "        Activation('relu'),\n",
    "\n",
    "        Dropout(dropout),\n",
    "        Dense(num_labels, kernel_regularizer=l2(0.001)),\n",
    "        Activation('softmax'),\n",
    "    ]\n",
    "    layers = block1 + block2 + block3 + backend\n",
    "    model = Sequential(layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_generator(data,audio_settings, batchsize):\n",
    "    \"\"\"\n",
    "    Keras generator for lazy-loading\n",
    "    data based on a pandas.DataFrame\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        idx = np.random.choice(len(data), size=batchsize, replace=False)\n",
    "\n",
    "        rows = data.iloc[idx, :].iterrows() #datailoc[idx, :].iterrows()[1]\n",
    "        mels = []\n",
    "        targets = []\n",
    "        for _, row in rows:\n",
    "            audio_p = get_audiop(row.filename) \n",
    "            mels.append(load_audio_windows(audio_p, audio_settings,True))\n",
    "            targets.append(row.target)\n",
    "        mels = np.asarray(mels)\n",
    "        categorical_targets = tf.keras.utils.to_categorical(targets, num_classes=50)\n",
    "        tf_ds = (mels, categorical_targets)\n",
    "        yield  tf_ds\n",
    "    '''\n",
    "    filename = row.filename\n",
    "    audio_path = get_audiop(filename)    \n",
    "    mels_arr = load_audio_windows(audio_path, audio_settings) \n",
    "    #reshape if needed\n",
    "    mels_arr = np.expand_dims(mels_arr, axis=-1)\n",
    "    batch = \n",
    "    target = row.target\n",
    "    tf_ds = (mels_arr, target) #bsize,n_mels,\n",
    "    '''\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dir(p: Path):\n",
    "    if p.is_dir():\n",
    "        print('models directory exists')\n",
    "    else:\n",
    "        p.mkdir(exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def train_model(out_dir,logs_dir, train, val, model, model_settings, audio_settings):\n",
    "    frame_samples = model_settings['hop_length']\n",
    "    window_frames = model_settings['frames']\n",
    "    epochs = model_settings['epochs']\n",
    "    batch_size = model_settings['batch']\n",
    "    lr = model_settings['lr']\n",
    "    momentum = model_settings['nesterov_momentum']\n",
    "    \n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(lr=lr, momentum=momentum,\n",
    "                                     nesterov=True)\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model_path = './' + str(out_dir) + \"/{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "    print('model_path: ',model_path)\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        model_path,\n",
    "        monitor='val_loss',\n",
    "        mode=\"auto\",\n",
    "        period=1,\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "    earlystop_callback = EarlyStopping(monitor='val_loss',\n",
    "                             patience=7,\n",
    "                            verbose=1,\n",
    "                            mode='auto')\n",
    "    logs_path ='./' + str(logs_dir)\n",
    "    tensorboard_callback = TensorBoard(log_dir=logs_path,\n",
    "                                       update_freq= 'epoch',\n",
    "                                       write_graph=True,\n",
    "                                       profile_batch=100000000)\n",
    "    lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "    \n",
    "    train_tfds = dataframe_generator(train, audio_settings, batch_size)\n",
    "    val_tfds = dataframe_generator(val, audio_settings, batch_size)\n",
    "\n",
    "    #tensorboard_callback\n",
    "    callbacks_list = [checkpoint,earlystop_callback,tensorboard_callback,lr_callback]\n",
    "    '''hist = model.fit_generator(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        steps_per_epoch=math.ceil(len(train) / batch_size),\n",
    "        validation_steps=math.ceil(len(val) / batch_size),\n",
    "        callbacks=callbacks_list,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "    )'''\n",
    "    \n",
    "    steps_per_epoch = len(train) // batch_size\n",
    "    validation_steps = len(val) // batch_size\n",
    "    \n",
    "    hist  = model.fit(x=train_tfds,\n",
    "                      epochs= epochs, verbose=1,\n",
    "                      callbacks=callbacks_list,\n",
    "                      steps_per_epoch= steps_per_epoch,\n",
    "                      validation_data= val_tfds ,\n",
    "                      validation_steps=validation_steps,\n",
    "                      shuffle=True, initial_epoch=0\n",
    "                     )\n",
    "    df = history_dataframe(hist)\n",
    "    history_path = os.path.join(out_dir, \"history.csv\")\n",
    "    df.to_csv(history_path)\n",
    "    \n",
    "    return hist\n",
    "    \n",
    "def history_dataframe(h):\n",
    "    data = {}\n",
    "    data[\"epoch\"] = h.epoch\n",
    "    for k, v in h.history.items():\n",
    "        data[k] = v\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def get_predict_batch(track):\n",
    "    while True:\n",
    "        mels=np.asarray([])\n",
    "        while len(mels)<10:\n",
    "            for window_step in range(250,track.shape[1],250):\n",
    "                if window_step + 250 > track.shape[1]:\n",
    "                    break\n",
    "                    \n",
    "                s_init = window_step - 250\n",
    "                s_end = window_step + 250\n",
    "                sample = track[:,s_init:s_end,:]\n",
    "                np.append(mels,sample)\n",
    "        print('Batch filled, batch shape = ', mels.shape)\n",
    "        yield mels\n",
    "        \n",
    "def predict_model( sample_path, audio_settings, model, method=\"mean\"):\n",
    "    print('Predicting....')\n",
    "    out = []\n",
    "    track = load_audio_windows(sample_path, audio_settings, True)\n",
    "    # shape (128,15214)\n",
    "    \n",
    "    batch = get_predict_batch(track)\n",
    "    print(\"Predicting Bath...\")\n",
    "    y_predict = model.predict(batch)\n",
    "    if method == \"mean\":\n",
    "        p = numpy.mean(predictions, axis=0)\n",
    "        assert len(p) == 10\n",
    "        out.append(p)\n",
    "    elif method == \"majority\":\n",
    "        votes = numpy.argmax(predictions, axis=1)\n",
    "        p = numpy.bincount(votes, minlength=10) / len(votes)\n",
    "    out.append(p)\n",
    "    \n",
    "    \n",
    "\n",
    "    ret = numpy.stack(out)\n",
    "    print(ret)\n",
    "    assert len(ret.shape) == 2, ret.shape\n",
    "    assert ret.shape[0] == len(out), ret.shape\n",
    "    assert ret.shape[1] == 50, ret.shape  # classes\n",
    "\n",
    "    return ret\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)    \n",
    "\n",
    "def main():\n",
    "    name = \"vh_challenge\"\n",
    "    output_dir= check_dir(MODEL_DIR)\n",
    "    logs_dir = check_dir(TBOARD_LOGS)\n",
    "    #Check data exists\n",
    "    \n",
    "    train, val = split_data(sdf)\n",
    "    print('Setup:')\n",
    "    print('Train size: ', len(train))\n",
    "    print('Validation size: ', len (val))\n",
    "    print('Epochs: ', model_settings['epochs'])\n",
    "    \n",
    "    m = build_model()\n",
    "    m.summary()\n",
    "    print(\"Training model\", name)\n",
    "\n",
    "    t= train_model(output_dir,logs_dir, train, val, m, model_settings, r_settings)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\\20200928-013914\\43-0.41.hdf5\n",
      "Building Model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-bad3e347aa1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_model\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mcompetition_track_path\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0maudio_settings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-9348cdae1d83>\u001b[0m in \u001b[0;36mpredict_model\u001b[1;34m(sample_path, audio_settings, model, method)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_predict_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vhc\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vhc\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vhc\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vhc\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vhc\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, standardize_function, workers, use_multiprocessing, max_queue_size, **kwargs)\u001b[0m\n\u001b[0;32m    745\u001b[0m     \u001b[1;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m     \u001b[1;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m     \u001b[0mpeek\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    748\u001b[0m     \u001b[0massert_not_namedtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vhc\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    848\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 850\u001b[1;33m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    851\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-9348cdae1d83>\u001b[0m in \u001b[0;36mget_predict_batch\u001b[1;34m(track)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[0ms_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwindow_step\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms_init\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ms_end\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mmels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\vhc\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   4669\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4670\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4671\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "competition_track_path = Path('rooster_competition.wav')\n",
    "model_path = Path('./model/20200928-013914/43-0.41.hdf5')\n",
    "print(model_path)\n",
    "model = build_model()\n",
    "\n",
    "loaded_model = load_model(str(model_path))\n",
    "prediction = predict_model( competition_track_path , audio_settings, loaded_model, method=\"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
